{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch stock data for taining from yahoo finance using lstm model\n",
    "## train the model\n",
    "## save the model\n",
    "#!Pip install yfinance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "import pickle\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "import os \n",
    "import pickle\n",
    "models_path = 'saved_models/'\n",
    "# fetch stock data from yahoo finance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pickle\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "\n",
    "def prepare_dataset(ticker, from_date, end_date, save_scaler=True):\n",
    "    # Download stock data with multiple features\n",
    "    dfs = []\n",
    "\n",
    "    data = yf.download(ticker, start=from_date, end=end_date)\n",
    "    data = data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Selecting OHLCV columns\n",
    "        ## append \n",
    "\n",
    "    # Append the prepared DataFrame to the list\n",
    "    dfs.append(data)\n",
    "    full_data = pd.concat(dfs)\n",
    "    ## shuffle the data\n",
    "    #full_data = full_data.sample(frac=1).reset_index(drop=True)\n",
    "    # Normalize the dat aset\n",
    "    features_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = features_scaler.fit_transform(full_data)\n",
    "    \n",
    "    target = full_data[['Close']]  # Target variable\n",
    "\n",
    "    # Normalize the target variable separately\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_target = target_scaler.fit_transform(target)\n",
    "\n",
    "    # Save the scalers for later use\n",
    "    if save_scaler:\n",
    "        pickle.dump(features_scaler, open(f'saved_models/{ticker}_features_scaler.pkl', 'wb'))\n",
    "        pickle.dump(target_scaler, open(f'saved_models/{ticker}_target_scaler.pkl', 'wb'))\n",
    "\n",
    "\n",
    "    return scaled_data, scaled_target\n",
    "\n",
    "def create_dataset_multifeature(data, scaled_target, time_step=1, prediction_step=10):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_step - prediction_step - 1):\n",
    "        X.append(data[i:(i + time_step), :])\n",
    "        # Y is now the 'Close' price at 'prediction_step' days ahead\n",
    "        Y.append(scaled_target[i + time_step + prediction_step - 1, 0])  # 0 index, assuming single column in scaled_target\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "def prepare_dataset_for_tickers(tickers, from_date, end_date, save_scaler=True):\n",
    "    dfs = []\n",
    "    for ticker in tickers:\n",
    "        # Download stock data\n",
    "        data = yf.download(ticker, start=from_date, end=end_date)\n",
    "        print(data)\n",
    "        data = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "        # data.index = pd.MultiIndex.from_product([[ticker], data.index], names=['Ticker', 'Date'])\n",
    "        dfs.append(data)\n",
    "    full_data = pd.concat(dfs)\n",
    "    print(full_data.head())\n",
    "    # Normalize features\n",
    "    features_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = features_scaler.fit_transform(full_data)\n",
    "\n",
    "    # Extract and normalize target ('Close')\n",
    "    target = full_data.loc[:, 'Close'].to_frame()\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_target = target_scaler.fit_transform(target)\n",
    "\n",
    "    if save_scaler:\n",
    "        pickle.dump(features_scaler, open('saved_models/{ticker}_features_scaler.pkl', 'wb'))\n",
    "        pickle.dump(target_scaler, open('saved_models/{ticker}_target_scaler.pkl', 'wb'))\n",
    "\n",
    "    return full_data, scaled_data, scaled_target\n",
    "\n",
    "def create_sequences_per_stock(data, scaled_target, time_step=1, prediction_step=10):\n",
    "    X, Y = [], []\n",
    "    unique_stocks = data.index.get_level_values(0).unique()\n",
    "    for stock in unique_stocks:\n",
    "        print(stock)\n",
    "        stock_data = data.loc[stock]\n",
    "        stock_scaled_target = scaled_target[data.index.get_level_values(0) == stock]\n",
    "        \n",
    "        for i in range(len(stock_data) - time_step - prediction_step + 1):\n",
    "            X.append(stock_data.iloc[i:(i + time_step)].values)\n",
    "            Y.append(stock_scaled_target[i + time_step + prediction_step - 1, 0])  # Assuming 'Close' as target\n",
    "    print(np.array(X).shape, np.array(Y).shape) \n",
    "    return shuffle(X, Y, random_state=42)  # np.array(X), np.array(Y)\n",
    "\n",
    "# Example of how to use these functions:\n",
    "\n",
    "def prepare_data_for_future_prediction(ticker, time_step, pred_step):\n",
    "    # Calculate start date with buffer for weekends/holidays and prediction step\n",
    "    today = datetime.now()\n",
    "    buffer_days = 2  # Adjust based on typical non-trading days in the market\n",
    "    start_date_buffer = (time_step + pred_step) * buffer_days\n",
    "    start_date = (today - timedelta(days=start_date_buffer)).strftime('%Y-%m-%d')\n",
    "    end_date = today.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Download the data\n",
    "    try:\n",
    "        data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        print(data.tail())\n",
    "        data = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "        print(len(data))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure the scaler used here is the same one used for model training\n",
    "    try:\n",
    "        scaler_path = f'saved_models/{ticker}_features_scaler.pkl'\n",
    "        scaler = pickle.load(open(scaler_path, 'rb'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading scaler: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Scaling the data\n",
    "    data_scaled = scaler.transform(data)\n",
    "    X_pred = np.array([data_scaled])\n",
    "    return X_pred   \n",
    "def get_model():\n",
    "        model = Sequential([\n",
    "        LSTM(256, return_sequences=True, input_shape=(100, 5)),  # 100 time steps, 5 features\n",
    "        LSTM(128),\n",
    "        Dense(1)\n",
    "    ])\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "# Assuming X and y datasets as prepared in the earlier step\n",
    "def train_lstm(model,ticker, X, y, epochs=50):\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 5))  # 5 for OHLCV features\n",
    "\n",
    "    # Define the LSTM model\n",
    "    # Fit the model\n",
    "    model.fit(X, y, epochs=epochs, batch_size=32, validation_split=0.2, verbose=2, callbacks=[early_stop])\n",
    "    #model.save(f'saved_models/{ticker}_lstm_model.keras')\n",
    "    return model\n",
    "\n",
    "# Usage example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 - 5s - 160ms/step - loss: 0.0241 - val_loss: 0.0032\n",
      "Epoch 2/30\n",
      "29/29 - 2s - 81ms/step - loss: 0.0098 - val_loss: 0.0014\n",
      "Epoch 3/30\n",
      "29/29 - 2s - 82ms/step - loss: 0.0092 - val_loss: 0.0010\n",
      "Epoch 4/30\n",
      "29/29 - 2s - 81ms/step - loss: 0.0098 - val_loss: 8.7517e-04\n",
      "Epoch 5/30\n",
      "29/29 - 2s - 81ms/step - loss: 0.0092 - val_loss: 9.5667e-04\n",
      "Epoch 6/30\n",
      "29/29 - 2s - 82ms/step - loss: 0.0097 - val_loss: 7.0219e-04\n",
      "Epoch 7/30\n",
      "29/29 - 2s - 82ms/step - loss: 0.0087 - val_loss: 0.0010\n",
      "Epoch 8/30\n",
      "29/29 - 2s - 83ms/step - loss: 0.0086 - val_loss: 7.2246e-04\n",
      "Epoch 9/30\n",
      "29/29 - 3s - 91ms/step - loss: 0.0088 - val_loss: 0.0014\n",
      "Epoch 10/30\n",
      "29/29 - 3s - 92ms/step - loss: 0.0083 - val_loss: 9.1566e-04\n",
      "Epoch 11/30\n",
      "29/29 - 3s - 93ms/step - loss: 0.0081 - val_loss: 0.0014\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "prev_5_yrs_date = (datetime.now() - timedelta(days=5*365)).strftime('%Y-%m-%d')\n",
    "time_step = 100\n",
    "pred_step = 10\n",
    "features, targets = prepare_dataset('IZO.CN', prev_5_yrs_date, current_date)\n",
    "X, y = create_dataset_multifeature(features, targets, time_step, pred_step)\n",
    "model = get_model()\n",
    "model = train_lstm(model,'IZO.CN', X, y, 30)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def train_gradientboosting():\n",
    "# Flatten X for Gradient Boosting\n",
    "    X_flat = X.reshape(X.shape[0], -1)\n",
    "\n",
    "    # Splitting dataset into training and testing set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_flat, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize and train the Gradient Boosting Regressor\n",
    "    gbr_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "    gbr_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions and evaluate\n",
    "    y_pred = gbr_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f'Test MSE: {mse}')\n",
    "    return gbr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_live_prediction(ticker,time_step, pred_step):\n",
    "    X_pred = prepare_data_for_future_prediction(ticker, time_step, pred_step)\n",
    "    target_path = f'saved_models\\{ticker}_target_scaler.pkl'\n",
    "    model_path = f'saved_models\\{ticker}_lstm_model.keras'\n",
    "    model = get_model()\n",
    "    model.load_weights(model_path)\n",
    "    ## load scaler \n",
    "    target_scaler = pickle.load(open(target_path, 'rb'))\n",
    "    ## load model\n",
    "    prediction = model.predict(X_pred)\n",
    "    transformed_pred = target_scaler.inverse_transform(prediction)\n",
    "    return transformed_pred\n",
    "y = do_live_prediction(\"IZO.CN\", time_step, pred_step)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Open  High   Low  Close  Volume\n",
      "Date                                       \n",
      "2024-04-03  0.15  0.15  0.15   0.15       0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m ticker_data \u001b[38;5;241m=\u001b[39m yf\u001b[38;5;241m.\u001b[39mTicker(ticker_symbol)\n\u001b[0;32m      9\u001b[0m ticker_df \u001b[38;5;241m=\u001b[39m ticker_data\u001b[38;5;241m.\u001b[39mhistory(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mticker_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "yes_date = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "data = yf.download(\"IZO.CN\", start=yes_date, end=current_date)\n",
    "data = data[['Open', 'High', 'Low', 'Close', 'Volume']]  # Selecting OHLCV columns\n",
    "print(data)\n",
    "        ## append \n",
    "ticker_symbol = \"IZO.CN\"\n",
    "ticker_data = yf.Ticker(ticker_symbol)\n",
    "ticker_df = ticker_data.history(period='5d')\n",
    "print(ticker_df[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IZO.CN: No price data found, symbol may be delisted (period=1d)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Open, High, Low, Close, Adj Close, Volume]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m ticker_df \u001b[38;5;241m=\u001b[39m ticker_data\u001b[38;5;241m.\u001b[39mhistory(period\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(ticker_df)\n\u001b[1;32m----> 5\u001b[0m current_price \u001b[38;5;241m=\u001b[39m \u001b[43mticker_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[1;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32mc:\\Users\\PC\\Documents\\GitHub\\StockPrediction-BioStocks\\.venv\\Lib\\site-packages\\pandas\\core\\indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "    ticker_symbol = \"IZO.CN\"\n",
    "    ticker_data = yf.Ticker(ticker_symbol)\n",
    "    ticker_df = ticker_data.history(period='1d')\n",
    "    print(ticker_df)\n",
    "    current_price = ticker_df['Close'].iloc[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gradientboosting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "\n",
    "\n",
    "# Assuming X and y datasets as prepared in the earlier step\n",
    "def train_lstm(X, y, ticker, epochs=50):\n",
    "# Reshape input to be [samples, time steps, features]\n",
    "    X = np.reshape(X, (X.shape[0], X.shape[1], 5))  # 5 for OHLCV features\n",
    "\n",
    "    # Define the LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(100, 5)),  # 100 time steps, 5 features\n",
    "        LSTM(50),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X, y, epochs=epochs, batch_size=32, validation_split=0.2, verbose=2, callbacks=[early_stop])\n",
    "    model.save(f'saved_models/{ticker}lstm_model.h5')\n",
    "    return model\n",
    "ticker = 'ATNM'\n",
    "model = train_lstm(X, y,ticker,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "pred_step = 30\n",
    "# Fetch current data\n",
    "buffer_days = 2  # This is an arbitrary value to increase the buffer for weekends and holidays\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=(time_step + pred_step) * buffer_days)\n",
    "\n",
    "# Format dates in YYYY-MM-DD format for yfinance\n",
    "start_date = start_date.strftime('%Y-%m-%d')\n",
    "end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "# Fetch data\n",
    "data = yf.download(ticker, start=start_date, end=end_date)\n",
    "data = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
    "\n",
    "# Prepare the latest batch of data for prediction\n",
    "scaled_data = scaler.transform(data)\n",
    "time_step = 100  # Assuming using the same time_step as during training\n",
    "latest_batch = scaled_data[-time_step:].reshape(1, time_step, data.shape[1])  # For LSTM\n",
    "latest_batch_flat = latest_batch.reshape(1, -1)  # For Gradient Boosting\n",
    "\n",
    "# Load models (Assuming they are saved and we are loading them for prediction)\n",
    "# lstm_model = load_model('path_to_saved_lstm_model.h5')\n",
    "# gbr_model = joblib.load('path_to_saved_gbr_model.pkl')  # Assuming Gradient Boosting model is saved with joblib\n",
    "\n",
    "# Predict with LSTM\n",
    "# predicted_price_lstm = lstm_model.predict(latest_batch)\n",
    "# Scale back the prediction to the original price range if necessary\n",
    "# predicted_price_lstm = scaler.inverse_transform(predicted_price_lstm)\n",
    "\n",
    "# Predict with Gradient Boosting Regressor\n",
    "# predicted_price_gbr = gbr_model.predict(latest_batch_flat)\n",
    "# Scale back the prediction to the original price range if necessary, depending on how the model was trained\n",
    "# e.g., predicted_price_gbr = scaler.inverse_transform(predicted_price_gbr.reshape(-1, 1))\n",
    "\n",
    "# Print predictions (remember to adjust inverse transformations based on your scaling and models)\n",
    "# print(f\"LSTM Predicted Price: {predicted_price_lstm}\")\n",
    "# print(f\"Gradient Boosting Predicted Price: {predicted_price_gbr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def prepare_dataset(dataset, pred_steps=0):\n",
    "    training_data_len = int(np.ceil( len(dataset) * .95 ))\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaled_data = scaler.fit_transform(dataset)\n",
    "    train_data = scaled_data[0:int(training_data_len), :]\n",
    "    # Split the data into x_train and y_train data sets\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "\n",
    "    for i in range(60, len(train_data) - pred_steps):\n",
    "        x_train.append(train_data[i-60:i, 0])\n",
    "        y_train.append(train_data[i+pred_steps, 0])\n",
    "        if i<= 61:\n",
    "            print(x_train)\n",
    "            print(y_train)\n",
    "            print()\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "    # Reshape the data\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "    # x_train.shape\n",
    "    # Create the testing data set\n",
    "    # Create a new array containing scaled values from index 1543 to 2002 \n",
    "    test_data = scaled_data[training_data_len - 60: , :]\n",
    "    # Create the data sets x_test and y_test\n",
    "    x_test = []\n",
    "    y_test = dataset[training_data_len:, :]\n",
    "    for i in range(60, len(test_data)):\n",
    "        x_test.append(test_data[i-60:i, 0])\n",
    "        \n",
    "    # Convert the data to a numpy array\n",
    "    x_test = np.array(x_test)\n",
    "\n",
    "    # Reshape the data\n",
    "    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "    # Get the models predicted price values \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, scaler\n",
    "\n",
    "def build_train_model(x_train, y_train, ticker, interval,interval_desc, model_name, save_model=True):\n",
    "    print(x_train.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "    ## path to appl stock model\n",
    "    saving_path = os.path.join(models_path, ticker, interval_desc, model_name +'.h5')\n",
    "    # model.save(saving_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data_train_model(ticker, start_date, end_date, interval,interval_desc,prediction_steps, epochs, batch_size, model_name):\n",
    "    # Get the data for Apple stock\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=interval)\n",
    "    # Print the first 5 rows\n",
    "    # Create a new dataframe with only the 'Close column \n",
    "    data = df.filter(['Close'])\n",
    "    # Convert the dataframe to a numpy array\n",
    "    dataset = data.values\n",
    "    # Get the number of rows to train the model on\n",
    "    x_train, y_train, x_test, y_test, scaler = prepare_dataset(dataset,prediction_steps)\n",
    "    print(x_train.shape[1])\n",
    "    return \n",
    "    # save scaler using pickle\n",
    "    scaler_path = os.path.join(models_path, ticker, interval_desc,'scaler.pkl')\n",
    "    with open('{}{}\\{}\\scaler.pkl'.format(models_path,ticker,interval_desc), 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    \n",
    "    # Build the LSTM model\n",
    "    model = build_train_model(x_train, y_train, ticker, interval,interval_desc, model_name, save_model=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GET CURRENT PRICE OF APPL STOCK USING YAHOO FINANCE\n",
    "# Get the data for Apple stock\n",
    "df = yf.download('AAPL', data_source='yahoo', start='2020-01-01', end='2020-03-20')\n",
    "\n",
    "current_price = df['Close'][-1]\n",
    "\n",
    "print(current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def get_saved_models(ticker,interval_desc):\n",
    "    # number of past ticks = 60 \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, return_sequences=True, input_shape= (60, 1)))\n",
    "    model.add(LSTM(64, return_sequences=False))\n",
    "    model.add(Dense(25))\n",
    "    model.add(Dense(1))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    # Train the model\n",
    "    model.load_weights('saved_models\\{}\\{}\\stock_prediction.h5'.format(ticker,interval_desc))\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_current_price(ticker):\n",
    "    date_before_60_days = datetime.datetime.now() - datetime.timedelta(days=100)\n",
    "\n",
    "    df = yf.download(ticker,  start=date_before_60_days, end=datetime.datetime.now() )\n",
    "    current_price = df['Close'][-1]\n",
    "    return current_price\n",
    "\n",
    "appl_model_hr = get_saved_models('AAPL','hr')\n",
    "appl_model_day = get_saved_models('AAPL','day')\n",
    "appl_model_week = get_saved_models('AAPL','week')\n",
    "app_hr_predcitions   = do_prediction(appl_model_hr, 'AAPL',interval='1h',interval_desc=\"hr\")[0][0]\n",
    "app_day_predcitions  = do_prediction(appl_model_day, 'AAPL',interval='1d',interval_desc=\"day\")[0][0]\n",
    "app_week_predcitions = do_prediction(appl_model_week, 'AAPL',interval='1d',interval_desc=\"week\")[0][0]\n",
    "current_aapl_price = get_current_price('AAPL')\n",
    "amzn_model_hr = get_saved_models('AMZN','hr')\n",
    "amzn_model_day = get_saved_models('AMZN','day')\n",
    "amzn_model_week = get_saved_models('AMZN','week')\n",
    "amzn_hr_predcitions   = do_prediction(appl_model_hr, 'AMZN',interval='1h',interval_desc=\"hr\")[0][0]\n",
    "amzn_day_predcitions  = do_prediction(appl_model_day, 'AMZN',interval='1d',interval_desc=\"day\")[0][0]\n",
    "amzn_week_predcitions = do_prediction(appl_model_week, 'AMZN',interval='1d',interval_desc=\"week\")[0][0]\n",
    "current_amzn_price = get_current_price('AMZN')\n",
    "\n",
    "print(current_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_prediction(model, ticker,interval,interval_desc):\n",
    "    ## fetch data of the last 60 days only\n",
    "# Get the stock quote\n",
    "    date_before_60_days = datetime.datetime.now() - datetime.timedelta(days=100)\n",
    "    df = yf.download(ticker, start=date_before_60_days, end=datetime.datetime.now(), interval=interval)\n",
    "\n",
    "\n",
    "    # Print the first 5 rows\n",
    "    print(df.head())\n",
    "\n",
    "    # Create a new dataframe with only the 'Close column \n",
    "    data = df.filter(['Close'])\n",
    "    # Convert the dataframe to a numpy array\n",
    "    dataset = data.values\n",
    "    len(dataset)\n",
    "    ## load scalar \n",
    "    scaler_path = os.path.join(models_path, ticker, interval_desc,'scaler.pkl')\n",
    "    with open(scaler_path, 'rb') as f:\n",
    "        scaler = pickle.load(f)\n",
    "    \n",
    "    scaled_data = scaler.transform(dataset)\n",
    "\n",
    "    x = scaled_data[-60:]\n",
    "    x = np.reshape(x, (1, x.shape[0], 1))\n",
    "    print(x.shape)\n",
    "    predictions = model.predict(x)\n",
    "    predictions = scaler.inverse_transform(predictions)\n",
    "    print(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.datetime.strptime('2022-12-31', '%Y-%m-%d')\n",
    "start_date = end_date - datetime.timedelta(days=600)\n",
    "df = yf.download('AAPL', start=start_date, end=end_date, interval='1h')\n",
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for next hr prediction \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date object for '2022-12-31'\n",
    "## convert string to date \n",
    "end_date = datetime.datetime.strptime('2022-12-31', '%Y-%m-%d')\n",
    "start_date = end_date - datetime.timedelta(days=600)\n",
    "ticker = 'AAPL'\n",
    "ticker = 'AMZN'\n",
    "trained_model = prepare_data_train_model(ticker=ticker,start_date= start_date, end_date=end_date, interval='1h',interval_desc=\"hr\", prediction_steps= 0,epochs= 1, batch_size= 1,model_name= 'stock_prediction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for next day prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = prepare_data_train_model(ticker=ticker,start_date= '2010-01-01', end_date='2022-12-31', interval='1d',interval_desc=\"day\", prediction_steps= 0,epochs= 1, batch_size= 1,model_name= 'stock_prediction')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model for next week prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = prepare_data_train_model(ticker=ticker,start_date= '2010-01-01', end_date='2022-12-31', interval='1d',interval_desc=\"week\", prediction_steps= 7,epochs= 1, batch_size= 1,model_name= 'stock_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model.load_weights('saved_models/AAPL/week/stock_prediction.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_prediction(trained_model, ticker='AMZN', interval='1h', interval_desc='hr')\n",
    "##152.60294\n",
    "##150.78731\n",
    "##152.50206"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training data for day prediction based on the last 60 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = scaled_data[0:int(training_data_len), :]\n",
    "# Split the data into x_train and y_train data sets\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    x_train.append(train_data[i-60:i, 0])\n",
    "    y_train.append(train_data[i, 0])\n",
    "    if i<= 61:\n",
    "        print(x_train)\n",
    "        print(y_train)\n",
    "        print()\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Reshape the data\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "# x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "interval = \"day\"\n",
    "stock = 'APPL'\n",
    "\n",
    "path = os.path.join(models_path, 'APPL', interval,'stock_prediction.h5')\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "interval = \"day\"\n",
    "stock = 'APPL'\n",
    "import os\n",
    "#path = os.path.join(models_path, 'APPL', interval,'stock_prediction.h5')\n",
    "#print(path)\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape= (x_train.shape[1], 1)))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=1, epochs=1)\n",
    "## path to appl stock model\n",
    "saving_path = os.path.join(models_path, stock, interval,'stock_prediction.h5')\n",
    "model.save(saving_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the testing data set\n",
    "# Create a new array containing scaled values from index 1543 to 2002 \n",
    "test_data = scaled_data[training_data_len - 60: , :]\n",
    "# Create the data sets x_test and y_test\n",
    "x_test = []\n",
    "y_test = dataset[training_data_len:, :]\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i-60:i, 0])\n",
    "    \n",
    "# Convert the data to a numpy array\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Reshape the data\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n",
    "\n",
    "# Get the models predicted price values \n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Get the root mean squared error (RMSE)\n",
    "rmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\n",
    "rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch data of the last 60 days only\n",
    "# Get the stock quote\n",
    "date_before_60_days = datetime.datetime.now() - datetime.timedelta(days=100)\n",
    "df = yf.download('AAPL', start=date_before_60_days, end=datetime.datetime.now(), interval=\"1d\")\n",
    "\n",
    "\n",
    "# Print the first 5 rows\n",
    "print(df.head())\n",
    "\n",
    "# Create a new dataframe with only the 'Close column \n",
    "data = df.filter(['Close'])\n",
    "# Convert the dataframe to a numpy array\n",
    "dataset = data.values\n",
    "len(dataset)\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "## predict the price of the next day based on the past 60 days\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "x = scaled_data[-60:]\n",
    "x = np.reshape(x, (1, x.shape[0], 1))\n",
    "print(x.shape)\n",
    "predictions = model.predict(x)\n",
    "predictions = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_excel('Komplette_Ãœbersicht.xlsx',read_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!Pip install openpyxl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b4c4d1d0443e1d64dc816c1734ac5eb5a5168c247e23e63025b90477d11ed63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
